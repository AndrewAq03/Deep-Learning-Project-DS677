{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLln9uQPYpIC"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# importing utilities\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "# importing data science libraries\n",
        "import pandas as pd\n",
        "import random as rd\n",
        "import numpy as np\n",
        "\n",
        "# importing pytorch libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import autograd\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# import visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "from IPython.display import Image, display\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "# ignore potential warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%matplotlib inline\n",
        "USE_CUDA = True\n",
        "# init deterministic seed\n",
        "seed_value = 1234 #4444 #3333 #2222 #1111 #1234\n",
        "rd.seed(seed_value) # set random seed\n",
        "np.random.seed(seed_value) # set numpy seed\n",
        "torch.manual_seed(seed_value) # set pytorch seed CPU\n",
        "if (torch.backends.cudnn.version() != None and USE_CUDA == True):\n",
        "    torch.cuda.manual_seed(seed_value) # set pytorch seed GPU\n",
        "\n",
        "# load the dataset into the notebook kernel\n",
        "ori_dataset = pd.read_csv('/content/fraud_dataset_v2.csv')\n",
        "\n",
        "# select categorical attributes to be \"one-hot\" encoded\n",
        "categorical_attr_names = ['KTOSL', 'PRCTR', 'BSCHL', 'HKONT']\n",
        "\n",
        "# encode categorical attributes into a binary one-hot encoded representation\n",
        "ori_dataset_categ_transformed = pd.get_dummies(ori_dataset[categorical_attr_names])\n",
        "\n",
        "\n",
        "# select categorical attributes to be \"one-hot\" encoded\n",
        "categorical_attr_names = ['KTOSL', 'PRCTR', 'BSCHL', 'HKONT']\n",
        "\n",
        "# encode categorical attributes into a binary one-hot encoded representation\n",
        "ori_dataset_categ_transformed = pd.get_dummies(ori_dataset[categorical_attr_names])\n",
        "\n",
        "# select \"DMBTR\" vs. \"WRBTR\" attribute\n",
        "numeric_attr_names = ['DMBTR', 'WRBTR']\n",
        "\n",
        "# add a small epsilon to eliminate zero values from data for log scaling\n",
        "numeric_attr = ori_dataset[numeric_attr_names] + 1e-7\n",
        "numeric_attr = numeric_attr.apply(np.log)\n",
        "\n",
        "# normalize all numeric attributes to the range [0,1]\n",
        "ori_dataset_numeric_attr = (numeric_attr - numeric_attr.min()) / (numeric_attr.max() - numeric_attr.min())\n",
        "\n",
        "# merge categorical and numeric subsets\n",
        "ori_subset_transformed = pd.concat([ori_dataset_categ_transformed, ori_dataset_numeric_attr], axis = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# implementation of the encoder network\n",
        "class encoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(encoder, self).__init__()\n",
        "\n",
        "        # specify layer 1 - in 618, out 512\n",
        "        self.encoder_L1 = nn.Linear(in_features=ori_subset_transformed.shape[1], out_features=512, bias=True) # add linearity\n",
        "        nn.init.xavier_uniform_(self.encoder_L1.weight) # init weights according to [9]\n",
        "        self.encoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
        "\n",
        "        # specify layer 2 - in 512, out 256\n",
        "        self.encoder_L2 = nn.Linear(512, 256, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L2.weight)\n",
        "        self.encoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 3 - in 256, out 128\n",
        "        self.encoder_L3 = nn.Linear(256, 128, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L3.weight)\n",
        "        self.encoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 4 - in 128, out 64\n",
        "        self.encoder_L4 = nn.Linear(128, 64, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L4.weight)\n",
        "        self.encoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 5 - in 64, out 32\n",
        "        self.encoder_L5 = nn.Linear(64, 32, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L5.weight)\n",
        "        self.encoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 6 - in 32, out 16\n",
        "        self.encoder_L6 = nn.Linear(32, 16, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L6.weight)\n",
        "        self.encoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 7 - in 16, out 8\n",
        "        self.encoder_L7 = nn.Linear(16, 8, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L7.weight)\n",
        "        self.encoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 8 - in 8, out 4\n",
        "        self.encoder_L8 = nn.Linear(8, 4, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L8.weight)\n",
        "        self.encoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 9 - in 4, out 3\n",
        "        self.encoder_L9 = nn.Linear(4, 3, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L9.weight)\n",
        "        self.encoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # init dropout layer with probability p\n",
        "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # define forward pass through the network\n",
        "        x = self.encoder_R1(self.dropout(self.encoder_L1(x)))\n",
        "        x = self.encoder_R2(self.dropout(self.encoder_L2(x)))\n",
        "        x = self.encoder_R3(self.dropout(self.encoder_L3(x)))\n",
        "        x = self.encoder_R4(self.dropout(self.encoder_L4(x)))\n",
        "        x = self.encoder_R5(self.dropout(self.encoder_L5(x)))\n",
        "        x = self.encoder_R6(self.dropout(self.encoder_L6(x)))\n",
        "        x = self.encoder_R7(self.dropout(self.encoder_L7(x)))\n",
        "        x = self.encoder_R8(self.dropout(self.encoder_L8(x)))\n",
        "        x = self.encoder_R9(self.encoder_L9(x)) # don't apply dropout to the AE bottleneck\n",
        "\n",
        "        return x\n",
        "\n",
        "# init training network classes / architectures\n",
        "encoder_train = encoder()\n",
        "\n",
        "# push to cuda if cudnn is available\n",
        "if (torch.backends.cudnn.version() != None and USE_CUDA == True):\n",
        "    encoder_train = encoder().cuda()\n"
      ],
      "metadata": {
        "id": "qJj0SBUpZNEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implementation of the decoder network\n",
        "class decoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(decoder, self).__init__()\n",
        "\n",
        "        # specify layer 1 - in 3, out 4\n",
        "        self.decoder_L1 = nn.Linear(in_features=3, out_features=4, bias=True) # add linearity\n",
        "        nn.init.xavier_uniform_(self.decoder_L1.weight)  # init weights according to [9]\n",
        "        self.decoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
        "\n",
        "        # specify layer 2 - in 4, out 8\n",
        "        self.decoder_L2 = nn.Linear(4, 8, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L2.weight)\n",
        "        self.decoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 3 - in 8, out 16\n",
        "        self.decoder_L3 = nn.Linear(8, 16, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L3.weight)\n",
        "        self.decoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 4 - in 16, out 32\n",
        "        self.decoder_L4 = nn.Linear(16, 32, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L4.weight)\n",
        "        self.decoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 5 - in 32, out 64\n",
        "        self.decoder_L5 = nn.Linear(32, 64, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L5.weight)\n",
        "        self.decoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 6 - in 64, out 128\n",
        "        self.decoder_L6 = nn.Linear(64, 128, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L6.weight)\n",
        "        self.decoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 7 - in 128, out 256\n",
        "        self.decoder_L7 = nn.Linear(128, 256, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L7.weight)\n",
        "        self.decoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 8 - in 256, out 512\n",
        "        self.decoder_L8 = nn.Linear(256, 512, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L8.weight)\n",
        "        self.decoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 9 - in 512, out 618\n",
        "        self.decoder_L9 = nn.Linear(in_features=512, out_features=ori_subset_transformed.shape[1], bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L9.weight)\n",
        "        self.decoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # init dropout layer with probability p\n",
        "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # define forward pass through the network\n",
        "        x = self.decoder_R1(self.dropout(self.decoder_L1(x)))\n",
        "        x = self.decoder_R2(self.dropout(self.decoder_L2(x)))\n",
        "        x = self.decoder_R3(self.dropout(self.decoder_L3(x)))\n",
        "        x = self.decoder_R4(self.dropout(self.decoder_L4(x)))\n",
        "        x = self.decoder_R5(self.dropout(self.decoder_L5(x)))\n",
        "        x = self.decoder_R6(self.dropout(self.decoder_L6(x)))\n",
        "        x = self.decoder_R7(self.dropout(self.decoder_L7(x)))\n",
        "        x = self.decoder_R8(self.dropout(self.decoder_L8(x)))\n",
        "        x = self.decoder_R9(self.decoder_L9(x)) # don't apply dropout to the AE output\n",
        "\n",
        "        return x\n",
        "\n",
        "# init training network classes / architectures\n",
        "decoder_train = decoder()\n",
        "\n",
        "# push to cuda if cudnn is available\n",
        "if (torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
        "    decoder_train = decoder().cuda()\n"
      ],
      "metadata": {
        "id": "6_IWZYjtZSU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the optimization criterion / loss function\n",
        "loss_function = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "# define learning rate and optimization strategy\n",
        "#learning_rate = 1e-3\n",
        "learning_rate = 1e-2\n",
        "encoder_optimizer = torch.optim.Adam(encoder_train.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = torch.optim.Adam(decoder_train.parameters(), lr=learning_rate)\n",
        "\n",
        "# specify training parameters\n",
        "#changed the number of epochs to 30\n",
        "#num_epochs = 30\n",
        "num_epochs = 5\n",
        "mini_batch_size = 128\n",
        "\n",
        "# convert pre-processed data to pytorch tensor\n",
        "#torch_dataset = torch.from_numpy(ori_subset_transformed.values).float()\n",
        "torch_dataset = torch.from_numpy(ori_subset_transformed.astype(np.float32).values).float() # Convert all columns to float32\n",
        "\n",
        "# convert to pytorch tensor - none cuda enabled\n",
        "dataloader = DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=True, num_workers=0)\n",
        "# note: we set num_workers to zero to retrieve deterministic results\n",
        "\n",
        "# determine if CUDA is available at compute node\n",
        "if (torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
        "    dataloader = DataLoader(torch_dataset.cuda(), batch_size=mini_batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "l_EcNTrAZaua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init collection of mini-batch losses\n",
        "losses = []\n",
        "\n",
        "# convert encoded transactional data to torch Variable\n",
        "data = autograd.Variable(torch_dataset)\n",
        "\n",
        "# train autoencoder model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # init mini batch counter\n",
        "    mini_batch_count = 0\n",
        "\n",
        "    # determine if CUDA is available at compute node\n",
        "    if(torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
        "\n",
        "        # set networks / models in GPU mode\n",
        "        encoder_train.cuda()\n",
        "        decoder_train.cuda()\n",
        "\n",
        "    # set networks in training mode (apply dropout when needed)\n",
        "    encoder_train.train()\n",
        "    decoder_train.train()\n",
        "\n",
        "    # start timer\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    # iterate over all mini-batches\n",
        "    for mini_batch_data in dataloader:\n",
        "\n",
        "        # increase mini batch counter\n",
        "        mini_batch_count += 1\n",
        "\n",
        "        # convert mini batch to torch variable\n",
        "        mini_batch_torch = autograd.Variable(mini_batch_data)\n",
        "\n",
        "        # =================== (1) forward pass ===================================\n",
        "\n",
        "        # run forward pass\n",
        "        z_representation = encoder_train(mini_batch_torch) # encode mini-batch data\n",
        "        mini_batch_reconstruction = decoder_train(z_representation) # decode mini-batch data\n",
        "\n",
        "        # =================== (2) compute reconstruction loss ====================\n",
        "\n",
        "        # determine reconstruction loss\n",
        "        reconstruction_loss = loss_function(mini_batch_reconstruction, mini_batch_torch)\n",
        "\n",
        "        # =================== (3) backward pass ==================================\n",
        "\n",
        "        # reset graph gradients\n",
        "        decoder_optimizer.zero_grad()\n",
        "        encoder_optimizer.zero_grad()\n",
        "\n",
        "        # run backward pass\n",
        "        reconstruction_loss.backward()\n",
        "\n",
        "        # =================== (4) update model parameters ========================\n",
        "\n",
        "        # update network parameters\n",
        "        decoder_optimizer.step()\n",
        "        encoder_optimizer.step()\n",
        "\n",
        "        # =================== monitor training progress ==========================\n",
        "\n",
        "        # print training progress each 1'000 mini-batches\n",
        "        if mini_batch_count % 1000 == 0:\n",
        "\n",
        "            # print the training mode: either on GPU or CPU\n",
        "            mode = 'GPU' if (torch.backends.cudnn.version() != None) and (USE_CUDA == True) else 'CPU'\n",
        "\n",
        "            # print mini batch reconstuction results\n",
        "            now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
        "            end_time = datetime.now() - start_time\n",
        "            print('[LOG {}] training status, epoch: [{:04}/{:04}], batch: {:04}, loss: {}, mode: {}, time required: {}'.format(now, (epoch+1), num_epochs, mini_batch_count, np.round(reconstruction_loss.item(), 4), mode, end_time))\n",
        "\n",
        "            # reset timer\n",
        "            start_time = datetime.now()\n",
        "\n",
        "    # =================== evaluate model performance =============================\n",
        "\n",
        "    # set networks in evaluation mode (don't apply dropout)\n",
        "    encoder_train.cpu().eval()\n",
        "    decoder_train.cpu().eval()\n",
        "\n",
        "    # reconstruct encoded transactional data\n",
        "    reconstruction = decoder_train(encoder_train(data))\n",
        "\n",
        "    # determine reconstruction loss - all transactions\n",
        "    reconstruction_loss_all = loss_function(reconstruction, data)\n",
        "\n",
        "    # collect reconstruction loss\n",
        "    losses.extend([reconstruction_loss_all.item()])\n",
        "\n",
        "    # print reconstuction loss results\n",
        "    now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
        "    print('[LOG {}] training status, epoch: [{:04}/{:04}], loss: {:.10f}'.format(now, (epoch+1), num_epochs, reconstruction_loss_all.item()))\n",
        "\n",
        "    # =================== save model snapshot to disk ============================\n",
        "    os.makedirs(\"/content/models\", exist_ok=True)\n",
        "    # # save trained encoder model file to disk\n",
        "    encoder_model_name = \"ep_{}_encoder_model.pth\".format((epoch+1))\n",
        "    torch.save(encoder_train.state_dict(), os.path.join(\"/content/models\", encoder_model_name))\n",
        "\n",
        "    # # save trained decoder model file to disk\n",
        "    decoder_model_name = \"ep_{}_decoder_model.pth\".format((epoch+1))\n",
        "    torch.save(decoder_train.state_dict(), os.path.join(\"/content/models\", decoder_model_name))\n",
        "\n",
        "# plot the training progress\n",
        "plt.plot(range(0, len(losses)), losses)\n",
        "plt.xlabel('[training epoch]')\n",
        "plt.xlim([0, len(losses)])\n",
        "plt.ylabel('[reconstruction-error]')\n",
        "#plt.ylim([0.0, 1.0])\n",
        "plt.title('AENN training performance')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944
        },
        "id": "jYi6EMM7ZgLq",
        "outputId": "0aa2ebeb-fc47-459a-cd98-b1e5f36be583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOG 20250511-18:36:27] training status, epoch: [0001/0005], batch: 1000, loss: 1552.0449, mode: GPU, time required: 0:00:05.455468\n",
            "[LOG 20250511-18:36:33] training status, epoch: [0001/0005], batch: 2000, loss: 22855.5996, mode: GPU, time required: 0:00:05.339948\n",
            "[LOG 20250511-18:36:38] training status, epoch: [0001/0005], batch: 3000, loss: 1572.3132, mode: GPU, time required: 0:00:05.364487\n",
            "[LOG 20250511-18:36:44] training status, epoch: [0001/0005], batch: 4000, loss: 619.701, mode: GPU, time required: 0:00:05.638185\n",
            "[LOG 20250511-18:36:48] training status, epoch: [0001/0005], loss: 412.1892700195\n",
            "[LOG 20250511-18:36:53] training status, epoch: [0002/0005], batch: 1000, loss: 387.275, mode: GPU, time required: 0:00:05.406158\n",
            "[LOG 20250511-18:36:59] training status, epoch: [0002/0005], batch: 2000, loss: 84.9628, mode: GPU, time required: 0:00:05.370957\n",
            "[LOG 20250511-18:37:04] training status, epoch: [0002/0005], batch: 3000, loss: 5027.5562, mode: GPU, time required: 0:00:05.485094\n",
            "[LOG 20250511-18:37:10] training status, epoch: [0002/0005], batch: 4000, loss: 76.7793, mode: GPU, time required: 0:00:05.528886\n",
            "[LOG 20250511-18:37:14] training status, epoch: [0002/0005], loss: 84.3270645142\n",
            "[LOG 20250511-18:37:19] training status, epoch: [0003/0005], batch: 1000, loss: 30.2352, mode: GPU, time required: 0:00:05.269342\n",
            "[LOG 20250511-18:37:25] training status, epoch: [0003/0005], batch: 2000, loss: 26.0743, mode: GPU, time required: 0:00:05.543329\n",
            "[LOG 20250511-18:37:30] training status, epoch: [0003/0005], batch: 3000, loss: 5.4462, mode: GPU, time required: 0:00:05.369168\n",
            "[LOG 20250511-18:37:36] training status, epoch: [0003/0005], batch: 4000, loss: 3.4686, mode: GPU, time required: 0:00:05.257039\n",
            "[LOG 20250511-18:37:40] training status, epoch: [0003/0005], loss: 8.2812118530\n",
            "[LOG 20250511-18:37:45] training status, epoch: [0004/0005], batch: 1000, loss: 3.5533, mode: GPU, time required: 0:00:05.382482\n",
            "[LOG 20250511-18:37:51] training status, epoch: [0004/0005], batch: 2000, loss: 1.9068, mode: GPU, time required: 0:00:05.311351\n",
            "[LOG 20250511-18:37:56] training status, epoch: [0004/0005], batch: 3000, loss: 47.981, mode: GPU, time required: 0:00:05.220405\n",
            "[LOG 20250511-18:38:01] training status, epoch: [0004/0005], batch: 4000, loss: 12.7573, mode: GPU, time required: 0:00:05.367460\n",
            "[LOG 20250511-18:38:05] training status, epoch: [0004/0005], loss: 15.8660182953\n",
            "[LOG 20250511-18:38:11] training status, epoch: [0005/0005], batch: 1000, loss: 6.0896, mode: GPU, time required: 0:00:05.369279\n",
            "[LOG 20250511-18:38:16] training status, epoch: [0005/0005], batch: 2000, loss: 7.8503, mode: GPU, time required: 0:00:05.254871\n",
            "[LOG 20250511-18:38:21] training status, epoch: [0005/0005], batch: 3000, loss: 2.9111, mode: GPU, time required: 0:00:05.281236\n",
            "[LOG 20250511-18:38:27] training status, epoch: [0005/0005], batch: 4000, loss: 4.2883, mode: GPU, time required: 0:00:05.331904\n",
            "[LOG 20250511-18:38:31] training status, epoch: [0005/0005], loss: 9.8192615509\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'AENN training performance')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# restore pretrained model checkpoint\n",
        "# encoder_model_name = \"ep_10_encoder_model.pth\"\n",
        "# decoder_model_name = \"ep_10_decoder_model.pth\"\n",
        "encoder_model_name = \"ep_5_encoder_model.pth\"\n",
        "decoder_model_name = \"ep_5_decoder_model.pth\"\n",
        "\n",
        "# init training network classes / architectures\n",
        "encoder_eval = encoder()\n",
        "decoder_eval = decoder()\n",
        "\n",
        "# load trained models\n",
        "encoder_eval.load_state_dict(torch.load(os.path.join(\"/content/models\", encoder_model_name)))\n",
        "decoder_eval.load_state_dict(torch.load(os.path.join(\"/content/models\", decoder_model_name)))"
      ],
      "metadata": {
        "id": "yHME62IvZodU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e0c5807-c2b5-4d39-d6fa-e75f052df337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert encoded transactional data to torch Variable\n",
        "data = autograd.Variable(torch_dataset)\n",
        "\n",
        "# set networks in evaluation mode (don't apply dropout)\n",
        "encoder_eval.eval()\n",
        "decoder_eval.eval()\n",
        "\n",
        "# reconstruct encoded transactional data\n",
        "reconstruction = decoder_eval(encoder_eval(data))"
      ],
      "metadata": {
        "id": "24hd0nh2ZrWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# determine reconstruction loss - all transactions\n",
        "reconstruction_loss_all = loss_function(reconstruction, data)\n",
        "\n",
        "# print reconstruction loss - all transactions\n",
        "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
        "print('[LOG {}] collected reconstruction loss of: {:06}/{:06} transactions'.format(now, reconstruction.size()[0], reconstruction.size()[0]))\n",
        "print('[LOG {}] reconstruction loss: {:.10f}'.format(now, reconstruction_loss_all.item()))\n"
      ],
      "metadata": {
        "id": "24CIqfivZt0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "537b0fd6-b37e-4264-e666-b6ad96751b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOG 20250511-18:39:10] collected reconstruction loss of: 533009/533009 transactions\n",
            "[LOG 20250511-18:39:10] reconstruction loss: 9.8192615509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init binary cross entropy errors\n",
        "reconstruction_loss_transaction = np.zeros(reconstruction.size()[0])\n",
        "\n",
        "# iterate over all detailed reconstructions\n",
        "for i in range(0, reconstruction.size()[0]):\n",
        "\n",
        "    # determine reconstruction loss - individual transactions\n",
        "    reconstruction_loss_transaction[i] = loss_function(reconstruction[i], data[i]).item()\n",
        "\n",
        "    if(i % 100000 == 0):\n",
        "\n",
        "        ### print conversion summary\n",
        "        now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
        "        print('[LOG {}] collected individual reconstruction loss of: {:06}/{:06} transactions'.format(now, i, reconstruction.size()[0]))"
      ],
      "metadata": {
        "id": "KiOs7Rx8Zvog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "157c61d6-0297-404f-9811-b1085e0ee6cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOG 20250511-18:39:11] collected individual reconstruction loss of: 000000/533009 transactions\n",
            "[LOG 20250511-18:39:17] collected individual reconstruction loss of: 100000/533009 transactions\n",
            "[LOG 20250511-18:39:22] collected individual reconstruction loss of: 200000/533009 transactions\n",
            "[LOG 20250511-18:39:27] collected individual reconstruction loss of: 300000/533009 transactions\n",
            "[LOG 20250511-18:39:33] collected individual reconstruction loss of: 400000/533009 transactions\n",
            "[LOG 20250511-18:39:38] collected individual reconstruction loss of: 500000/533009 transactions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init binary cross entropy errors\n",
        "reconstruction_loss_transaction = np.zeros(reconstruction.size()[0])\n",
        "\n",
        "# iterate over all detailed reconstructions\n",
        "for i in range(0, reconstruction.size()[0]):\n",
        "\n",
        "    # determine reconstruction loss - individual transactions\n",
        "    reconstruction_loss_transaction[i] = loss_function(reconstruction[i], data[i]).item()\n",
        "\n",
        "    if(i % 100000 == 0):\n",
        "\n",
        "        ### print conversion summary\n",
        "        now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
        "        print('[LOG {}] collected individual reconstruction loss of: {:06}/{:06} transactions'.format(now, i, reconstruction.size()[0]))"
      ],
      "metadata": {
        "id": "8h7SHdreZxZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19f6743d-d5d2-49be-d281-a268ae013c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOG 20250511-18:39:43] collected individual reconstruction loss of: 000000/533009 transactions\n",
            "[LOG 20250511-18:39:48] collected individual reconstruction loss of: 100000/533009 transactions\n",
            "[LOG 20250511-18:39:54] collected individual reconstruction loss of: 200000/533009 transactions\n",
            "[LOG 20250511-18:39:59] collected individual reconstruction loss of: 300000/533009 transactions\n",
            "[LOG 20250511-18:40:05] collected individual reconstruction loss of: 400000/533009 transactions\n",
            "[LOG 20250511-18:40:10] collected individual reconstruction loss of: 500000/533009 transactions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, load your validation dataset\n",
        "val_dataset = pd.read_csv('fraud_dataset_v1.csv')\n",
        "\n",
        "# Select categorical attributes to be \"one-hot\" encoded\n",
        "categorical_attr_names = ['BELNR', 'BSCHL', 'HKONT', 'DMBTR']\n",
        "val_categ_transformed = pd.get_dummies(val_dataset[categorical_attr_names])\n",
        "\n",
        "# Ensure all categories from training are present in validation (add missing columns with 0s)\n",
        "missing_cols = set(ori_dataset_categ_transformed.columns) - set(val_categ_transformed.columns)\n",
        "for col in missing_cols:\n",
        "    val_categ_transformed[col] = 0\n",
        "val_categ_transformed = val_categ_transformed[ori_dataset_categ_transformed.columns]  # Reorder columns to match training\n",
        "\n",
        "# Process numeric attributes\n",
        "val_numeric_attr = val_dataset[numeric_attr_names] + 1e-7\n",
        "val_numeric_attr = val_numeric_attr.apply(np.log)\n",
        "\n",
        "# Normalize using the same min/max as training data\n",
        "val_numeric_attr = (val_numeric_attr - numeric_attr.min()) / (numeric_attr.max() - numeric_attr.min())\n",
        "\n",
        "# Merge categorical and numeric subsets\n",
        "val_subset_transformed = pd.concat([val_categ_transformed, val_numeric_attr], axis=1)\n",
        "\n",
        "# Convert to PyTorch tensor\n",
        "val_tensor = torch.from_numpy(val_subset_transformed.astype(np.float32).values).float()\n",
        "\n",
        "# Push to GPU if available\n",
        "if (torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
        "    val_tensor = val_tensor.cuda()\n",
        "\n",
        "# Evaluation function with proper device handling\n",
        "def evaluate_on_validation_set(encoder, decoder, val_data):\n",
        "    # Set models to evaluation mode\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    # Determine device\n",
        "    device = next(encoder.parameters()).device  # Get device from model parameters\n",
        "\n",
        "    # Move data to the same device as models\n",
        "    val_data = val_data.to(device)\n",
        "\n",
        "    total_loss = 0\n",
        "    individual_losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Process in batches to avoid memory issues\n",
        "        batch_size = 1024  # You can adjust this based on your GPU memory\n",
        "        num_samples = val_data.size(0)\n",
        "\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            batch = val_data[i:i+batch_size]\n",
        "            batch_var = autograd.Variable(batch)\n",
        "\n",
        "            # Forward pass\n",
        "            z_representation = encoder(batch_var)\n",
        "            reconstruction = decoder(z_representation)\n",
        "\n",
        "            # Compute loss for this batch\n",
        "            batch_loss = loss_function(reconstruction, batch_var)\n",
        "            total_loss += batch_loss.item() * batch.size(0)\n",
        "\n",
        "            # Compute individual losses\n",
        "            for j in range(batch.size(0)):\n",
        "                individual_loss = loss_function(reconstruction[j], batch_var[j]).item()\n",
        "                individual_losses.append(individual_loss)\n",
        "\n",
        "            if i % (10 * batch_size) == 0:  # Print progress every 10 batches\n",
        "                now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
        "                print(f'[LOG {now}] Processed {i:06}/{num_samples:06} validation samples')\n",
        "\n",
        "    # Calculate average loss\n",
        "    avg_loss = total_loss / num_samples\n",
        "\n",
        "    return avg_loss, np.array(individual_losses)\n",
        "\n",
        "# Before evaluation, ensure models are on the correct device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() and USE_CUDA else \"cpu\")\n",
        "encoder_train.to(device)\n",
        "decoder_train.to(device)\n",
        "val_tensor = val_tensor.to(device)\n",
        "\n",
        "# Now evaluate on validation set\n",
        "val_loss, val_individual_losses = evaluate_on_validation_set(encoder_train, decoder_train, val_tensor)\n",
        "\n",
        "# Print results\n",
        "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
        "print(f'[LOG {now}] Validation set reconstruction loss: {val_loss:.10f}')\n",
        "\n",
        "# Analysis\n",
        "print(f\"\\nValidation Set Analysis:\")\n",
        "print(f\"Mean loss: {np.mean(val_individual_losses):.6f}\")\n",
        "print(f\"Std dev: {np.std(val_individual_losses):.6f}\")\n",
        "print(f\"Min loss: {np.min(val_individual_losses):.6f}\")\n",
        "print(f\"Max loss: {np.max(val_individual_losses):.6f}\")\n",
        "print(f\"Median loss: {np.median(val_individual_losses):.6f}\")\n",
        "\n",
        "\n",
        "# Anomaly detection\n",
        "threshold = np.percentile(val_individual_losses, 95)\n",
        "anomaly_indices = np.where(val_individual_losses > threshold)[0]\n",
        "print(f\"\\nFound {len(anomaly_indices)} potential anomalies (top 5% highest errors) in validation set\")"
      ],
      "metadata": {
        "id": "vUoQUfzzhbei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46f9264b-2392-4ca6-8dc6-2d9b403dd9b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOG 20250511-18:59:53] Processed 000000/533009 validation samples\n",
            "[LOG 20250511-18:59:54] Processed 010240/533009 validation samples\n",
            "[LOG 20250511-18:59:55] Processed 020480/533009 validation samples\n",
            "[LOG 20250511-18:59:56] Processed 030720/533009 validation samples\n",
            "[LOG 20250511-18:59:57] Processed 040960/533009 validation samples\n",
            "[LOG 20250511-18:59:58] Processed 051200/533009 validation samples\n",
            "[LOG 20250511-18:59:59] Processed 061440/533009 validation samples\n",
            "[LOG 20250511-19:00:00] Processed 071680/533009 validation samples\n",
            "[LOG 20250511-19:00:01] Processed 081920/533009 validation samples\n",
            "[LOG 20250511-19:00:02] Processed 092160/533009 validation samples\n",
            "[LOG 20250511-19:00:03] Processed 102400/533009 validation samples\n",
            "[LOG 20250511-19:00:04] Processed 112640/533009 validation samples\n",
            "[LOG 20250511-19:00:04] Processed 122880/533009 validation samples\n",
            "[LOG 20250511-19:00:05] Processed 133120/533009 validation samples\n",
            "[LOG 20250511-19:00:06] Processed 143360/533009 validation samples\n",
            "[LOG 20250511-19:00:07] Processed 153600/533009 validation samples\n",
            "[LOG 20250511-19:00:08] Processed 163840/533009 validation samples\n",
            "[LOG 20250511-19:00:09] Processed 174080/533009 validation samples\n",
            "[LOG 20250511-19:00:10] Processed 184320/533009 validation samples\n",
            "[LOG 20250511-19:00:11] Processed 194560/533009 validation samples\n",
            "[LOG 20250511-19:00:12] Processed 204800/533009 validation samples\n",
            "[LOG 20250511-19:00:13] Processed 215040/533009 validation samples\n",
            "[LOG 20250511-19:00:14] Processed 225280/533009 validation samples\n",
            "[LOG 20250511-19:00:15] Processed 235520/533009 validation samples\n",
            "[LOG 20250511-19:00:16] Processed 245760/533009 validation samples\n",
            "[LOG 20250511-19:00:17] Processed 256000/533009 validation samples\n",
            "[LOG 20250511-19:00:18] Processed 266240/533009 validation samples\n",
            "[LOG 20250511-19:00:19] Processed 276480/533009 validation samples\n",
            "[LOG 20250511-19:00:20] Processed 286720/533009 validation samples\n",
            "[LOG 20250511-19:00:21] Processed 296960/533009 validation samples\n",
            "[LOG 20250511-19:00:22] Processed 307200/533009 validation samples\n",
            "[LOG 20250511-19:00:23] Processed 317440/533009 validation samples\n",
            "[LOG 20250511-19:00:24] Processed 327680/533009 validation samples\n",
            "[LOG 20250511-19:00:25] Processed 337920/533009 validation samples\n",
            "[LOG 20250511-19:00:26] Processed 348160/533009 validation samples\n",
            "[LOG 20250511-19:00:27] Processed 358400/533009 validation samples\n",
            "[LOG 20250511-19:00:28] Processed 368640/533009 validation samples\n",
            "[LOG 20250511-19:00:29] Processed 378880/533009 validation samples\n",
            "[LOG 20250511-19:00:30] Processed 389120/533009 validation samples\n",
            "[LOG 20250511-19:00:30] Processed 399360/533009 validation samples\n",
            "[LOG 20250511-19:00:31] Processed 409600/533009 validation samples\n",
            "[LOG 20250511-19:00:32] Processed 419840/533009 validation samples\n",
            "[LOG 20250511-19:00:33] Processed 430080/533009 validation samples\n",
            "[LOG 20250511-19:00:34] Processed 440320/533009 validation samples\n",
            "[LOG 20250511-19:00:35] Processed 450560/533009 validation samples\n",
            "[LOG 20250511-19:00:36] Processed 460800/533009 validation samples\n",
            "[LOG 20250511-19:00:37] Processed 471040/533009 validation samples\n",
            "[LOG 20250511-19:00:38] Processed 481280/533009 validation samples\n",
            "[LOG 20250511-19:00:39] Processed 491520/533009 validation samples\n",
            "[LOG 20250511-19:00:40] Processed 501760/533009 validation samples\n",
            "[LOG 20250511-19:00:41] Processed 512000/533009 validation samples\n",
            "[LOG 20250511-19:00:42] Processed 522240/533009 validation samples\n",
            "[LOG 20250511-19:00:43] Processed 532480/533009 validation samples\n",
            "[LOG 20250511-19:00:43] Validation set reconstruction loss: 12.4406764034\n",
            "\n",
            "Validation Set Analysis:\n",
            "Mean loss: 12.440676\n",
            "Std dev: 44.347311\n",
            "Min loss: 1.794441\n",
            "Max loss: 28242.718750\n",
            "Median loss: 6.753364\n",
            "\n",
            "Found 26651 potential anomalies (top 5% highest errors) in validation set\n"
          ]
        }
      ]
    }
  ]
}