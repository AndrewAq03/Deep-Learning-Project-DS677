{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZLln9uQPYpIC"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# importing utilities\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "# importing data science libraries\n",
        "import pandas as pd\n",
        "import random as rd\n",
        "import numpy as np\n",
        "\n",
        "# importing pytorch libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import autograd\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# import visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "from IPython.display import Image, display\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "# ignore potential warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%matplotlib inline\n",
        "USE_CUDA = True\n",
        "# init deterministic seed\n",
        "seed_value = 1234 #4444 #3333 #2222 #1111 #1234\n",
        "rd.seed(seed_value) # set random seed\n",
        "np.random.seed(seed_value) # set numpy seed\n",
        "torch.manual_seed(seed_value) # set pytorch seed CPU\n",
        "if (torch.backends.cudnn.version() != None and USE_CUDA == True):\n",
        "    torch.cuda.manual_seed(seed_value) # set pytorch seed GPU\n",
        "\n",
        "# load the dataset into the notebook kernel\n",
        "ori_dataset = pd.read_csv('/content/fraud_dataset_v2.csv')\n",
        "\n",
        "# select categorical attributes to be \"one-hot\" encoded\n",
        "categorical_attr_names = ['KTOSL', 'PRCTR', 'BSCHL', 'HKONT']\n",
        "\n",
        "# encode categorical attributes into a binary one-hot encoded representation\n",
        "ori_dataset_categ_transformed = pd.get_dummies(ori_dataset[categorical_attr_names])\n",
        "\n",
        "\n",
        "# select categorical attributes to be \"one-hot\" encoded\n",
        "categorical_attr_names = ['KTOSL', 'PRCTR', 'BSCHL', 'HKONT']\n",
        "\n",
        "# encode categorical attributes into a binary one-hot encoded representation\n",
        "ori_dataset_categ_transformed = pd.get_dummies(ori_dataset[categorical_attr_names])\n",
        "\n",
        "# select \"DMBTR\" vs. \"WRBTR\" attribute\n",
        "numeric_attr_names = ['DMBTR', 'WRBTR']\n",
        "\n",
        "# add a small epsilon to eliminate zero values from data for log scaling\n",
        "numeric_attr = ori_dataset[numeric_attr_names] + 1e-7\n",
        "numeric_attr = numeric_attr.apply(np.log)\n",
        "\n",
        "# normalize all numeric attributes to the range [0,1]\n",
        "ori_dataset_numeric_attr = (numeric_attr - numeric_attr.min()) / (numeric_attr.max() - numeric_attr.min())\n",
        "\n",
        "# merge categorical and numeric subsets\n",
        "ori_subset_transformed = pd.concat([ori_dataset_categ_transformed, ori_dataset_numeric_attr], axis = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# implementation of the encoder network\n",
        "class encoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(encoder, self).__init__()\n",
        "\n",
        "        # specify layer 1 - in 618, out 512\n",
        "        self.encoder_L1 = nn.Linear(in_features=ori_subset_transformed.shape[1], out_features=512, bias=True) # add linearity\n",
        "        nn.init.xavier_uniform_(self.encoder_L1.weight) # init weights according to [9]\n",
        "        self.encoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
        "\n",
        "        # specify layer 2 - in 512, out 256\n",
        "        self.encoder_L2 = nn.Linear(512, 256, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L2.weight)\n",
        "        self.encoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 3 - in 256, out 128\n",
        "        self.encoder_L3 = nn.Linear(256, 128, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L3.weight)\n",
        "        self.encoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 4 - in 128, out 64\n",
        "        self.encoder_L4 = nn.Linear(128, 64, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L4.weight)\n",
        "        self.encoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 5 - in 64, out 32\n",
        "        self.encoder_L5 = nn.Linear(64, 32, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L5.weight)\n",
        "        self.encoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 6 - in 32, out 16\n",
        "        self.encoder_L6 = nn.Linear(32, 16, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L6.weight)\n",
        "        self.encoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 7 - in 16, out 8\n",
        "        self.encoder_L7 = nn.Linear(16, 8, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L7.weight)\n",
        "        self.encoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 8 - in 8, out 4\n",
        "        self.encoder_L8 = nn.Linear(8, 4, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L8.weight)\n",
        "        self.encoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 9 - in 4, out 3\n",
        "        self.encoder_L9 = nn.Linear(4, 3, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L9.weight)\n",
        "        self.encoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # init dropout layer with probability p\n",
        "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # define forward pass through the network\n",
        "        x = self.encoder_R1(self.dropout(self.encoder_L1(x)))\n",
        "        x = self.encoder_R2(self.dropout(self.encoder_L2(x)))\n",
        "        x = self.encoder_R3(self.dropout(self.encoder_L3(x)))\n",
        "        x = self.encoder_R4(self.dropout(self.encoder_L4(x)))\n",
        "        x = self.encoder_R5(self.dropout(self.encoder_L5(x)))\n",
        "        x = self.encoder_R6(self.dropout(self.encoder_L6(x)))\n",
        "        x = self.encoder_R7(self.dropout(self.encoder_L7(x)))\n",
        "        x = self.encoder_R8(self.dropout(self.encoder_L8(x)))\n",
        "        x = self.encoder_R9(self.encoder_L9(x)) # don't apply dropout to the AE bottleneck\n",
        "\n",
        "        return x\n",
        "\n",
        "# init training network classes / architectures\n",
        "encoder_train = encoder()\n",
        "\n",
        "# push to cuda if cudnn is available\n",
        "if (torch.backends.cudnn.version() != None and USE_CUDA == True):\n",
        "    encoder_train = encoder().cuda()\n"
      ],
      "metadata": {
        "id": "qJj0SBUpZNEe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implementation of the decoder network\n",
        "class decoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(decoder, self).__init__()\n",
        "\n",
        "        # specify layer 1 - in 3, out 4\n",
        "        self.decoder_L1 = nn.Linear(in_features=3, out_features=4, bias=True) # add linearity\n",
        "        nn.init.xavier_uniform_(self.decoder_L1.weight)  # init weights according to [9]\n",
        "        self.decoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
        "\n",
        "        # specify layer 2 - in 4, out 8\n",
        "        self.decoder_L2 = nn.Linear(4, 8, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L2.weight)\n",
        "        self.decoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 3 - in 8, out 16\n",
        "        self.decoder_L3 = nn.Linear(8, 16, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L3.weight)\n",
        "        self.decoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 4 - in 16, out 32\n",
        "        self.decoder_L4 = nn.Linear(16, 32, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L4.weight)\n",
        "        self.decoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 5 - in 32, out 64\n",
        "        self.decoder_L5 = nn.Linear(32, 64, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L5.weight)\n",
        "        self.decoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 6 - in 64, out 128\n",
        "        self.decoder_L6 = nn.Linear(64, 128, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L6.weight)\n",
        "        self.decoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 7 - in 128, out 256\n",
        "        self.decoder_L7 = nn.Linear(128, 256, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L7.weight)\n",
        "        self.decoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 8 - in 256, out 512\n",
        "        self.decoder_L8 = nn.Linear(256, 512, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L8.weight)\n",
        "        self.decoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 9 - in 512, out 618\n",
        "        self.decoder_L9 = nn.Linear(in_features=512, out_features=ori_subset_transformed.shape[1], bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L9.weight)\n",
        "        self.decoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # init dropout layer with probability p\n",
        "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # define forward pass through the network\n",
        "        x = self.decoder_R1(self.dropout(self.decoder_L1(x)))\n",
        "        x = self.decoder_R2(self.dropout(self.decoder_L2(x)))\n",
        "        x = self.decoder_R3(self.dropout(self.decoder_L3(x)))\n",
        "        x = self.decoder_R4(self.dropout(self.decoder_L4(x)))\n",
        "        x = self.decoder_R5(self.dropout(self.decoder_L5(x)))\n",
        "        x = self.decoder_R6(self.dropout(self.decoder_L6(x)))\n",
        "        x = self.decoder_R7(self.dropout(self.decoder_L7(x)))\n",
        "        x = self.decoder_R8(self.dropout(self.decoder_L8(x)))\n",
        "        x = self.decoder_R9(self.decoder_L9(x)) # don't apply dropout to the AE output\n",
        "\n",
        "        return x\n",
        "\n",
        "# init training network classes / architectures\n",
        "decoder_train = decoder()\n",
        "\n",
        "# push to cuda if cudnn is available\n",
        "if (torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
        "    decoder_train = decoder().cuda()\n"
      ],
      "metadata": {
        "id": "6_IWZYjtZSU6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the optimization criterion / loss function\n",
        "loss_function = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "# define learning rate and optimization strategy\n",
        "learning_rate = 1e-3\n",
        "encoder_optimizer = torch.optim.Adam(encoder_train.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = torch.optim.Adam(decoder_train.parameters(), lr=learning_rate)\n",
        "\n",
        "# specify training parameters\n",
        "#changed the number of epochs to 30\n",
        "num_epochs = 30\n",
        "mini_batch_size = 128\n",
        "\n",
        "# convert pre-processed data to pytorch tensor\n",
        "#torch_dataset = torch.from_numpy(ori_subset_transformed.values).float()\n",
        "torch_dataset = torch.from_numpy(ori_subset_transformed.astype(np.float32).values).float() # Convert all columns to float32\n",
        "\n",
        "# convert to pytorch tensor - none cuda enabled\n",
        "dataloader = DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=True, num_workers=0)\n",
        "# note: we set num_workers to zero to retrieve deterministic results\n",
        "\n",
        "# determine if CUDA is available at compute node\n",
        "if (torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
        "    dataloader = DataLoader(torch_dataset.cuda(), batch_size=mini_batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "l_EcNTrAZaua"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init collection of mini-batch losses\n",
        "losses = []\n",
        "\n",
        "# convert encoded transactional data to torch Variable\n",
        "data = autograd.Variable(torch_dataset)\n",
        "\n",
        "# train autoencoder model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # init mini batch counter\n",
        "    mini_batch_count = 0\n",
        "\n",
        "    # determine if CUDA is available at compute node\n",
        "    if(torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
        "\n",
        "        # set networks / models in GPU mode\n",
        "        encoder_train.cuda()\n",
        "        decoder_train.cuda()\n",
        "\n",
        "    # set networks in training mode (apply dropout when needed)\n",
        "    encoder_train.train()\n",
        "    decoder_train.train()\n",
        "\n",
        "    # start timer\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    # iterate over all mini-batches\n",
        "    for mini_batch_data in dataloader:\n",
        "\n",
        "        # increase mini batch counter\n",
        "        mini_batch_count += 1\n",
        "\n",
        "        # convert mini batch to torch variable\n",
        "        mini_batch_torch = autograd.Variable(mini_batch_data)\n",
        "\n",
        "        # =================== (1) forward pass ===================================\n",
        "\n",
        "        # run forward pass\n",
        "        z_representation = encoder_train(mini_batch_torch) # encode mini-batch data\n",
        "        mini_batch_reconstruction = decoder_train(z_representation) # decode mini-batch data\n",
        "\n",
        "        # =================== (2) compute reconstruction loss ====================\n",
        "\n",
        "        # determine reconstruction loss\n",
        "        reconstruction_loss = loss_function(mini_batch_reconstruction, mini_batch_torch)\n",
        "\n",
        "        # =================== (3) backward pass ==================================\n",
        "\n",
        "        # reset graph gradients\n",
        "        decoder_optimizer.zero_grad()\n",
        "        encoder_optimizer.zero_grad()\n",
        "\n",
        "        # run backward pass\n",
        "        reconstruction_loss.backward()\n",
        "\n",
        "        # =================== (4) update model parameters ========================\n",
        "\n",
        "        # update network parameters\n",
        "        decoder_optimizer.step()\n",
        "        encoder_optimizer.step()\n",
        "\n",
        "        # =================== monitor training progress ==========================\n",
        "\n",
        "        # print training progress each 1'000 mini-batches\n",
        "        if mini_batch_count % 1000 == 0:\n",
        "\n",
        "            # print the training mode: either on GPU or CPU\n",
        "            mode = 'GPU' if (torch.backends.cudnn.version() != None) and (USE_CUDA == True) else 'CPU'\n",
        "\n",
        "            # print mini batch reconstuction results\n",
        "            now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
        "            end_time = datetime.now() - start_time\n",
        "            print('[LOG {}] training status, epoch: [{:04}/{:04}], batch: {:04}, loss: {}, mode: {}, time required: {}'.format(now, (epoch+1), num_epochs, mini_batch_count, np.round(reconstruction_loss.item(), 4), mode, end_time))\n",
        "\n",
        "            # reset timer\n",
        "            start_time = datetime.now()\n",
        "\n",
        "    # =================== evaluate model performance =============================\n",
        "\n",
        "    # set networks in evaluation mode (don't apply dropout)\n",
        "    encoder_train.cpu().eval()\n",
        "    decoder_train.cpu().eval()\n",
        "\n",
        "    # reconstruct encoded transactional data\n",
        "    reconstruction = decoder_train(encoder_train(data))\n",
        "\n",
        "    # determine reconstruction loss - all transactions\n",
        "    reconstruction_loss_all = loss_function(reconstruction, data)\n",
        "\n",
        "    # collect reconstruction loss\n",
        "    losses.extend([reconstruction_loss_all.item()])\n",
        "\n",
        "    # print reconstuction loss results\n",
        "    now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
        "    print('[LOG {}] training status, epoch: [{:04}/{:04}], loss: {:.10f}'.format(now, (epoch+1), num_epochs, reconstruction_loss_all.item()))\n",
        "\n",
        "    # =================== save model snapshot to disk ============================\n",
        "\n",
        "    # save trained encoder model file to disk\n",
        "    encoder_model_name = \"ep_{}_encoder_model.pth\".format((epoch+1))\n",
        "    torch.save(encoder_train.state_dict(), os.path.join(\"/content/models\", encoder_model_name))\n",
        "\n",
        "    # save trained decoder model file to disk\n",
        "    decoder_model_name = \"ep_{}_decoder_model.pth\".format((epoch+1))\n",
        "    torch.save(decoder_train.state_dict(), os.path.join(\"/content/models\", decoder_model_name))\n",
        "\n",
        "# plot the training progress\n",
        "plt.plot(range(0, len(losses)), losses)\n",
        "plt.xlabel('[training epoch]')\n",
        "plt.xlim([0, len(losses)])\n",
        "plt.ylabel('[reconstruction-error]')\n",
        "#plt.ylim([0.0, 1.0])\n",
        "plt.title('AENN training performance')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jYi6EMM7ZgLq",
        "outputId": "db44e1e2-7f26-44a2-f59b-b2650459b18e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOG 20250508-01:08:53] training status, epoch: [0001/0030], batch: 1000, loss: 0.0107, mode: GPU, time required: 0:00:05.319803\n",
            "[LOG 20250508-01:08:58] training status, epoch: [0001/0030], batch: 2000, loss: 0.0067, mode: GPU, time required: 0:00:05.335266\n",
            "[LOG 20250508-01:09:03] training status, epoch: [0001/0030], batch: 3000, loss: 0.007, mode: GPU, time required: 0:00:05.266183\n",
            "[LOG 20250508-01:09:09] training status, epoch: [0001/0030], batch: 4000, loss: 0.0049, mode: GPU, time required: 0:00:05.281235\n",
            "[LOG 20250508-01:09:13] training status, epoch: [0001/0030], loss: 0.0060353428\n",
            "[LOG 20250508-01:09:18] training status, epoch: [0002/0030], batch: 1000, loss: 0.0046, mode: GPU, time required: 0:00:05.322566\n",
            "[LOG 20250508-01:09:24] training status, epoch: [0002/0030], batch: 2000, loss: 0.0069, mode: GPU, time required: 0:00:05.293764\n",
            "[LOG 20250508-01:09:29] training status, epoch: [0002/0030], batch: 3000, loss: 0.0041, mode: GPU, time required: 0:00:05.305033\n",
            "[LOG 20250508-01:09:34] training status, epoch: [0002/0030], batch: 4000, loss: 0.0038, mode: GPU, time required: 0:00:05.332932\n",
            "[LOG 20250508-01:09:39] training status, epoch: [0002/0030], loss: 0.0042509693\n",
            "[LOG 20250508-01:09:44] training status, epoch: [0003/0030], batch: 1000, loss: 0.0037, mode: GPU, time required: 0:00:05.404415\n",
            "[LOG 20250508-01:09:50] training status, epoch: [0003/0030], batch: 2000, loss: 0.0036, mode: GPU, time required: 0:00:05.289446\n",
            "[LOG 20250508-01:09:55] training status, epoch: [0003/0030], batch: 3000, loss: 0.0038, mode: GPU, time required: 0:00:05.323552\n",
            "[LOG 20250508-01:10:00] training status, epoch: [0003/0030], batch: 4000, loss: 0.0032, mode: GPU, time required: 0:00:05.319311\n",
            "[LOG 20250508-01:10:05] training status, epoch: [0003/0030], loss: 0.0039287913\n",
            "[LOG 20250508-01:10:10] training status, epoch: [0004/0030], batch: 1000, loss: 0.0037, mode: GPU, time required: 0:00:05.366034\n",
            "[LOG 20250508-01:10:16] training status, epoch: [0004/0030], batch: 2000, loss: 0.0033, mode: GPU, time required: 0:00:05.303107\n",
            "[LOG 20250508-01:10:21] training status, epoch: [0004/0030], batch: 3000, loss: 0.0033, mode: GPU, time required: 0:00:05.332242\n",
            "[LOG 20250508-01:10:26] training status, epoch: [0004/0030], batch: 4000, loss: 0.0031, mode: GPU, time required: 0:00:05.323095\n",
            "[LOG 20250508-01:10:31] training status, epoch: [0004/0030], loss: 0.0033211892\n",
            "[LOG 20250508-01:10:37] training status, epoch: [0005/0030], batch: 1000, loss: 0.0039, mode: GPU, time required: 0:00:05.357084\n",
            "[LOG 20250508-01:10:42] training status, epoch: [0005/0030], batch: 2000, loss: 0.0034, mode: GPU, time required: 0:00:05.268423\n",
            "[LOG 20250508-01:10:47] training status, epoch: [0005/0030], batch: 3000, loss: 0.0029, mode: GPU, time required: 0:00:05.243453\n",
            "[LOG 20250508-01:10:52] training status, epoch: [0005/0030], batch: 4000, loss: 0.0028, mode: GPU, time required: 0:00:05.223367\n",
            "[LOG 20250508-01:10:57] training status, epoch: [0005/0030], loss: 0.0029234912\n",
            "[LOG 20250508-01:11:02] training status, epoch: [0006/0030], batch: 1000, loss: 0.0027, mode: GPU, time required: 0:00:05.374780\n",
            "[LOG 20250508-01:11:08] training status, epoch: [0006/0030], batch: 2000, loss: 0.003, mode: GPU, time required: 0:00:05.336046\n",
            "[LOG 20250508-01:11:13] training status, epoch: [0006/0030], batch: 3000, loss: 0.0032, mode: GPU, time required: 0:00:05.328743\n",
            "[LOG 20250508-01:11:19] training status, epoch: [0006/0030], batch: 4000, loss: 0.0027, mode: GPU, time required: 0:00:05.393845\n",
            "[LOG 20250508-01:11:23] training status, epoch: [0006/0030], loss: 0.0027913686\n",
            "[LOG 20250508-01:11:28] training status, epoch: [0007/0030], batch: 1000, loss: 0.0029, mode: GPU, time required: 0:00:05.376574\n",
            "[LOG 20250508-01:11:34] training status, epoch: [0007/0030], batch: 2000, loss: 0.0029, mode: GPU, time required: 0:00:05.274899\n",
            "[LOG 20250508-01:11:39] training status, epoch: [0007/0030], batch: 3000, loss: 0.0028, mode: GPU, time required: 0:00:05.276016\n",
            "[LOG 20250508-01:11:44] training status, epoch: [0007/0030], batch: 4000, loss: 0.0028, mode: GPU, time required: 0:00:05.299589\n",
            "[LOG 20250508-01:11:49] training status, epoch: [0007/0030], loss: 0.0026917481\n",
            "[LOG 20250508-01:11:54] training status, epoch: [0008/0030], batch: 1000, loss: 0.0027, mode: GPU, time required: 0:00:05.379719\n",
            "[LOG 20250508-01:12:00] training status, epoch: [0008/0030], batch: 2000, loss: 0.0027, mode: GPU, time required: 0:00:05.260016\n",
            "[LOG 20250508-01:12:05] training status, epoch: [0008/0030], batch: 3000, loss: 0.0026, mode: GPU, time required: 0:00:05.304457\n",
            "[LOG 20250508-01:12:10] training status, epoch: [0008/0030], batch: 4000, loss: 0.0025, mode: GPU, time required: 0:00:05.333492\n",
            "[LOG 20250508-01:12:15] training status, epoch: [0008/0030], loss: 0.0026148204\n",
            "[LOG 20250508-01:12:20] training status, epoch: [0009/0030], batch: 1000, loss: 0.0025, mode: GPU, time required: 0:00:05.378329\n",
            "[LOG 20250508-01:12:26] training status, epoch: [0009/0030], batch: 2000, loss: 0.0026, mode: GPU, time required: 0:00:05.341525\n",
            "[LOG 20250508-01:12:31] training status, epoch: [0009/0030], batch: 3000, loss: 0.0024, mode: GPU, time required: 0:00:05.283826\n",
            "[LOG 20250508-01:12:36] training status, epoch: [0009/0030], batch: 4000, loss: 0.0026, mode: GPU, time required: 0:00:05.291977\n",
            "[LOG 20250508-01:12:41] training status, epoch: [0009/0030], loss: 0.0026374743\n",
            "[LOG 20250508-01:12:46] training status, epoch: [0010/0030], batch: 1000, loss: 0.0025, mode: GPU, time required: 0:00:05.321639\n",
            "[LOG 20250508-01:12:52] training status, epoch: [0010/0030], batch: 2000, loss: 0.0028, mode: GPU, time required: 0:00:05.332988\n",
            "[LOG 20250508-01:12:57] training status, epoch: [0010/0030], batch: 3000, loss: 0.0031, mode: GPU, time required: 0:00:05.273143\n",
            "[LOG 20250508-01:13:02] training status, epoch: [0010/0030], batch: 4000, loss: 0.0025, mode: GPU, time required: 0:00:05.332887\n",
            "[LOG 20250508-01:13:07] training status, epoch: [0010/0030], loss: 0.0108992681\n",
            "[LOG 20250508-01:13:12] training status, epoch: [0011/0030], batch: 1000, loss: 0.0034, mode: GPU, time required: 0:00:05.355930\n",
            "[LOG 20250508-01:13:17] training status, epoch: [0011/0030], batch: 2000, loss: 0.0026, mode: GPU, time required: 0:00:05.241563\n",
            "[LOG 20250508-01:13:23] training status, epoch: [0011/0030], batch: 3000, loss: 0.0025, mode: GPU, time required: 0:00:05.399863\n",
            "[LOG 20250508-01:13:28] training status, epoch: [0011/0030], batch: 4000, loss: 0.0025, mode: GPU, time required: 0:00:05.516818\n",
            "[LOG 20250508-01:13:33] training status, epoch: [0011/0030], loss: 0.0025318507\n",
            "[LOG 20250508-01:13:38] training status, epoch: [0012/0030], batch: 1000, loss: 0.0025, mode: GPU, time required: 0:00:05.380051\n",
            "[LOG 20250508-01:13:43] training status, epoch: [0012/0030], batch: 2000, loss: 0.0025, mode: GPU, time required: 0:00:05.302222\n",
            "[LOG 20250508-01:13:49] training status, epoch: [0012/0030], batch: 3000, loss: 0.0025, mode: GPU, time required: 0:00:05.328321\n",
            "[LOG 20250508-01:13:54] training status, epoch: [0012/0030], batch: 4000, loss: 0.0025, mode: GPU, time required: 0:00:05.276262\n",
            "[LOG 20250508-01:13:59] training status, epoch: [0012/0030], loss: 0.0028738480\n",
            "[LOG 20250508-01:14:04] training status, epoch: [0013/0030], batch: 1000, loss: 0.0026, mode: GPU, time required: 0:00:05.334079\n",
            "[LOG 20250508-01:14:09] training status, epoch: [0013/0030], batch: 2000, loss: 0.0025, mode: GPU, time required: 0:00:05.275874\n",
            "[LOG 20250508-01:14:14] training status, epoch: [0013/0030], batch: 3000, loss: 0.0024, mode: GPU, time required: 0:00:05.301246\n",
            "[LOG 20250508-01:14:20] training status, epoch: [0013/0030], batch: 4000, loss: 0.0026, mode: GPU, time required: 0:00:05.259114\n",
            "[LOG 20250508-01:14:24] training status, epoch: [0013/0030], loss: 0.0024805737\n",
            "[LOG 20250508-01:14:30] training status, epoch: [0014/0030], batch: 1000, loss: 0.0024, mode: GPU, time required: 0:00:05.315685\n",
            "[LOG 20250508-01:14:35] training status, epoch: [0014/0030], batch: 2000, loss: 0.0028, mode: GPU, time required: 0:00:05.303109\n",
            "[LOG 20250508-01:14:40] training status, epoch: [0014/0030], batch: 3000, loss: 0.0025, mode: GPU, time required: 0:00:05.319088\n",
            "[LOG 20250508-01:14:46] training status, epoch: [0014/0030], batch: 4000, loss: 0.0025, mode: GPU, time required: 0:00:05.314224\n",
            "[LOG 20250508-01:14:50] training status, epoch: [0014/0030], loss: 0.0024691783\n",
            "[LOG 20250508-01:14:55] training status, epoch: [0015/0030], batch: 1000, loss: 0.0024, mode: GPU, time required: 0:00:05.367674\n",
            "[LOG 20250508-01:15:01] training status, epoch: [0015/0030], batch: 2000, loss: 0.0024, mode: GPU, time required: 0:00:05.327617\n",
            "[LOG 20250508-01:15:06] training status, epoch: [0015/0030], batch: 3000, loss: 0.0025, mode: GPU, time required: 0:00:05.323390\n",
            "[LOG 20250508-01:15:11] training status, epoch: [0015/0030], batch: 4000, loss: 0.0026, mode: GPU, time required: 0:00:05.319611\n",
            "[LOG 20250508-01:15:16] training status, epoch: [0015/0030], loss: 0.0024890543\n",
            "[LOG 20250508-01:15:21] training status, epoch: [0016/0030], batch: 1000, loss: 0.0024, mode: GPU, time required: 0:00:05.385593\n",
            "[LOG 20250508-01:15:26] training status, epoch: [0016/0030], batch: 2000, loss: 0.0047, mode: GPU, time required: 0:00:05.242891\n",
            "[LOG 20250508-01:15:32] training status, epoch: [0016/0030], batch: 3000, loss: 0.0025, mode: GPU, time required: 0:00:05.281908\n",
            "[LOG 20250508-01:15:37] training status, epoch: [0016/0030], batch: 4000, loss: 0.0025, mode: GPU, time required: 0:00:05.286650\n",
            "[LOG 20250508-01:15:41] training status, epoch: [0016/0030], loss: 0.0029928631\n",
            "[LOG 20250508-01:15:47] training status, epoch: [0017/0030], batch: 1000, loss: 0.0025, mode: GPU, time required: 0:00:05.353718\n",
            "[LOG 20250508-01:15:52] training status, epoch: [0017/0030], batch: 2000, loss: 0.0024, mode: GPU, time required: 0:00:05.271481\n",
            "[LOG 20250508-01:15:57] training status, epoch: [0017/0030], batch: 3000, loss: 0.0024, mode: GPU, time required: 0:00:05.318802\n",
            "[LOG 20250508-01:16:03] training status, epoch: [0017/0030], batch: 4000, loss: 0.0036, mode: GPU, time required: 0:00:05.334694\n",
            "[LOG 20250508-01:16:07] training status, epoch: [0017/0030], loss: 0.0024622399\n",
            "[LOG 20250508-01:16:13] training status, epoch: [0018/0030], batch: 1000, loss: 0.0023, mode: GPU, time required: 0:00:05.360122\n",
            "[LOG 20250508-01:16:18] training status, epoch: [0018/0030], batch: 2000, loss: 0.0025, mode: GPU, time required: 0:00:05.323507\n",
            "[LOG 20250508-01:16:24] training status, epoch: [0018/0030], batch: 3000, loss: 0.0024, mode: GPU, time required: 0:00:05.325878\n",
            "[LOG 20250508-01:16:29] training status, epoch: [0018/0030], batch: 4000, loss: 0.0026, mode: GPU, time required: 0:00:05.356453\n",
            "[LOG 20250508-01:16:33] training status, epoch: [0018/0030], loss: 0.0024328025\n",
            "[LOG 20250508-01:16:39] training status, epoch: [0019/0030], batch: 1000, loss: 0.0026, mode: GPU, time required: 0:00:05.378189\n",
            "[LOG 20250508-01:16:44] training status, epoch: [0019/0030], batch: 2000, loss: 0.0024, mode: GPU, time required: 0:00:05.304400\n",
            "[LOG 20250508-01:16:49] training status, epoch: [0019/0030], batch: 3000, loss: 0.0032, mode: GPU, time required: 0:00:05.372122\n",
            "[LOG 20250508-01:16:55] training status, epoch: [0019/0030], batch: 4000, loss: 0.0023, mode: GPU, time required: 0:00:05.379996\n",
            "[LOG 20250508-01:16:59] training status, epoch: [0019/0030], loss: 0.0025678773\n",
            "[LOG 20250508-01:17:05] training status, epoch: [0020/0030], batch: 1000, loss: 0.0024, mode: GPU, time required: 0:00:05.369355\n",
            "[LOG 20250508-01:17:10] training status, epoch: [0020/0030], batch: 2000, loss: 0.0024, mode: GPU, time required: 0:00:05.280769\n",
            "[LOG 20250508-01:17:15] training status, epoch: [0020/0030], batch: 3000, loss: 0.0025, mode: GPU, time required: 0:00:05.323471\n",
            "[LOG 20250508-01:17:20] training status, epoch: [0020/0030], batch: 4000, loss: 0.0023, mode: GPU, time required: 0:00:05.329438\n",
            "[LOG 20250508-01:17:25] training status, epoch: [0020/0030], loss: 0.0025646114\n",
            "[LOG 20250508-01:17:30] training status, epoch: [0021/0030], batch: 1000, loss: 0.0023, mode: GPU, time required: 0:00:05.414340\n",
            "[LOG 20250508-01:17:36] training status, epoch: [0021/0030], batch: 2000, loss: 0.0024, mode: GPU, time required: 0:00:05.339802\n",
            "[LOG 20250508-01:17:41] training status, epoch: [0021/0030], batch: 3000, loss: 0.0024, mode: GPU, time required: 0:00:05.318595\n",
            "[LOG 20250508-01:17:46] training status, epoch: [0021/0030], batch: 4000, loss: 0.0024, mode: GPU, time required: 0:00:05.316903\n",
            "[LOG 20250508-01:17:51] training status, epoch: [0021/0030], loss: 0.0023834659\n",
            "[LOG 20250508-01:17:56] training status, epoch: [0022/0030], batch: 1000, loss: 0.0023, mode: GPU, time required: 0:00:05.344883\n",
            "[LOG 20250508-01:18:02] training status, epoch: [0022/0030], batch: 2000, loss: 0.0027, mode: GPU, time required: 0:00:05.352406\n",
            "[LOG 20250508-01:18:07] training status, epoch: [0022/0030], batch: 3000, loss: 0.0024, mode: GPU, time required: 0:00:05.334504\n",
            "[LOG 20250508-01:18:12] training status, epoch: [0022/0030], batch: 4000, loss: 0.0036, mode: GPU, time required: 0:00:05.382362\n",
            "[LOG 20250508-01:18:17] training status, epoch: [0022/0030], loss: 0.0024492596\n",
            "[LOG 20250508-01:18:22] training status, epoch: [0023/0030], batch: 1000, loss: 0.0022, mode: GPU, time required: 0:00:05.338965\n",
            "[LOG 20250508-01:18:27] training status, epoch: [0023/0030], batch: 2000, loss: 0.0024, mode: GPU, time required: 0:00:05.356476\n",
            "[LOG 20250508-01:18:33] training status, epoch: [0023/0030], batch: 3000, loss: 0.0023, mode: GPU, time required: 0:00:05.302546\n",
            "[LOG 20250508-01:18:38] training status, epoch: [0023/0030], batch: 4000, loss: 0.0022, mode: GPU, time required: 0:00:05.363899\n",
            "[LOG 20250508-01:18:42] training status, epoch: [0023/0030], loss: 0.0023714590\n",
            "[LOG 20250508-01:18:48] training status, epoch: [0024/0030], batch: 1000, loss: 0.0023, mode: GPU, time required: 0:00:05.380096\n",
            "[LOG 20250508-01:18:53] training status, epoch: [0024/0030], batch: 2000, loss: 0.0036, mode: GPU, time required: 0:00:05.270225\n",
            "[LOG 20250508-01:18:58] training status, epoch: [0024/0030], batch: 3000, loss: 0.0022, mode: GPU, time required: 0:00:05.330590\n",
            "[LOG 20250508-01:19:04] training status, epoch: [0024/0030], batch: 4000, loss: 0.0023, mode: GPU, time required: 0:00:05.265359\n",
            "[LOG 20250508-01:19:08] training status, epoch: [0024/0030], loss: 0.0023099266\n",
            "[LOG 20250508-01:19:13] training status, epoch: [0025/0030], batch: 1000, loss: 0.0023, mode: GPU, time required: 0:00:05.332990\n",
            "[LOG 20250508-01:19:19] training status, epoch: [0025/0030], batch: 2000, loss: 0.0024, mode: GPU, time required: 0:00:05.311932\n",
            "[LOG 20250508-01:19:24] training status, epoch: [0025/0030], batch: 3000, loss: 0.0023, mode: GPU, time required: 0:00:05.369964\n",
            "[LOG 20250508-01:19:29] training status, epoch: [0025/0030], batch: 4000, loss: 0.0023, mode: GPU, time required: 0:00:05.331271\n",
            "[LOG 20250508-01:19:34] training status, epoch: [0025/0030], loss: 0.0022912880\n",
            "[LOG 20250508-01:19:40] training status, epoch: [0026/0030], batch: 1000, loss: 0.0022, mode: GPU, time required: 0:00:05.354113\n",
            "[LOG 20250508-01:19:45] training status, epoch: [0026/0030], batch: 2000, loss: 0.0029, mode: GPU, time required: 0:00:05.308107\n",
            "[LOG 20250508-01:19:50] training status, epoch: [0026/0030], batch: 3000, loss: 0.0023, mode: GPU, time required: 0:00:05.232177\n",
            "[LOG 20250508-01:19:55] training status, epoch: [0026/0030], batch: 4000, loss: 0.0024, mode: GPU, time required: 0:00:05.328699\n",
            "[LOG 20250508-01:20:00] training status, epoch: [0026/0030], loss: 0.0022888072\n",
            "[LOG 20250508-01:20:05] training status, epoch: [0027/0030], batch: 1000, loss: 0.0024, mode: GPU, time required: 0:00:05.298373\n",
            "[LOG 20250508-01:20:10] training status, epoch: [0027/0030], batch: 2000, loss: 0.0024, mode: GPU, time required: 0:00:05.317546\n",
            "[LOG 20250508-01:20:16] training status, epoch: [0027/0030], batch: 3000, loss: 0.0023, mode: GPU, time required: 0:00:05.277594\n",
            "[LOG 20250508-01:20:21] training status, epoch: [0027/0030], batch: 4000, loss: 0.0023, mode: GPU, time required: 0:00:05.286653\n",
            "[LOG 20250508-01:20:25] training status, epoch: [0027/0030], loss: 0.0022938023\n",
            "[LOG 20250508-01:20:31] training status, epoch: [0028/0030], batch: 1000, loss: 0.0023, mode: GPU, time required: 0:00:05.318168\n",
            "[LOG 20250508-01:20:36] training status, epoch: [0028/0030], batch: 2000, loss: 0.0024, mode: GPU, time required: 0:00:05.320909\n",
            "[LOG 20250508-01:20:41] training status, epoch: [0028/0030], batch: 3000, loss: 0.0023, mode: GPU, time required: 0:00:05.383103\n",
            "[LOG 20250508-01:20:47] training status, epoch: [0028/0030], batch: 4000, loss: 0.0023, mode: GPU, time required: 0:00:05.311518\n",
            "[LOG 20250508-01:20:51] training status, epoch: [0028/0030], loss: 0.0022875261\n",
            "[LOG 20250508-01:20:56] training status, epoch: [0029/0030], batch: 1000, loss: 0.0022, mode: GPU, time required: 0:00:05.300474\n",
            "[LOG 20250508-01:21:02] training status, epoch: [0029/0030], batch: 2000, loss: 0.0022, mode: GPU, time required: 0:00:05.206763\n",
            "[LOG 20250508-01:21:07] training status, epoch: [0029/0030], batch: 3000, loss: 0.0024, mode: GPU, time required: 0:00:05.321792\n",
            "[LOG 20250508-01:21:12] training status, epoch: [0029/0030], batch: 4000, loss: 0.0022, mode: GPU, time required: 0:00:05.303106\n",
            "[LOG 20250508-01:21:17] training status, epoch: [0029/0030], loss: 0.0022822747\n",
            "[LOG 20250508-01:21:22] training status, epoch: [0030/0030], batch: 1000, loss: 0.0023, mode: GPU, time required: 0:00:05.320617\n",
            "[LOG 20250508-01:21:28] training status, epoch: [0030/0030], batch: 2000, loss: 0.0023, mode: GPU, time required: 0:00:05.300737\n",
            "[LOG 20250508-01:21:33] training status, epoch: [0030/0030], batch: 3000, loss: 0.0023, mode: GPU, time required: 0:00:05.298657\n",
            "[LOG 20250508-01:21:38] training status, epoch: [0030/0030], batch: 4000, loss: 0.0024, mode: GPU, time required: 0:00:05.340250\n",
            "[LOG 20250508-01:21:43] training status, epoch: [0030/0030], loss: 0.0022748099\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'AENN training performance')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# restore pretrained model checkpoint\n",
        "# encoder_model_name = \"ep_10_encoder_model.pth\"\n",
        "# decoder_model_name = \"ep_10_decoder_model.pth\"\n",
        "encoder_model_name = \"ep_5_encoder_model.pth\"\n",
        "decoder_model_name = \"ep_5_decoder_model.pth\"\n",
        "\n",
        "# init training network classes / architectures\n",
        "encoder_eval = encoder()\n",
        "decoder_eval = decoder()\n",
        "\n",
        "# load trained models\n",
        "encoder_eval.load_state_dict(torch.load(os.path.join(\"/content/models\", encoder_model_name)))\n",
        "decoder_eval.load_state_dict(torch.load(os.path.join(\"/content/models\", decoder_model_name)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHME62IvZodU",
        "outputId": "03cb96eb-efa4-4b95-c8d2-4ec801a8cfa3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert encoded transactional data to torch Variable\n",
        "data = autograd.Variable(torch_dataset)\n",
        "\n",
        "# set networks in evaluation mode (don't apply dropout)\n",
        "encoder_eval.eval()\n",
        "decoder_eval.eval()\n",
        "\n",
        "# reconstruct encoded transactional data\n",
        "reconstruction = decoder_eval(encoder_eval(data))"
      ],
      "metadata": {
        "id": "24hd0nh2ZrWR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# determine reconstruction loss - all transactions\n",
        "reconstruction_loss_all = loss_function(reconstruction, data)\n",
        "\n",
        "# print reconstruction loss - all transactions\n",
        "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
        "print('[LOG {}] collected reconstruction loss of: {:06}/{:06} transactions'.format(now, reconstruction.size()[0], reconstruction.size()[0]))\n",
        "print('[LOG {}] reconstruction loss: {:.10f}'.format(now, reconstruction_loss_all.item()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24CIqfivZt0y",
        "outputId": "295533a1-d069-4ade-d614-a325d20d6dfb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOG 20250508-01:21:47] collected reconstruction loss of: 533009/533009 transactions\n",
            "[LOG 20250508-01:21:47] reconstruction loss: 0.0029234912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init binary cross entropy errors\n",
        "reconstruction_loss_transaction = np.zeros(reconstruction.size()[0])\n",
        "\n",
        "# iterate over all detailed reconstructions\n",
        "for i in range(0, reconstruction.size()[0]):\n",
        "\n",
        "    # determine reconstruction loss - individual transactions\n",
        "    reconstruction_loss_transaction[i] = loss_function(reconstruction[i], data[i]).item()\n",
        "\n",
        "    if(i % 100000 == 0):\n",
        "\n",
        "        ### print conversion summary\n",
        "        now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
        "        print('[LOG {}] collected individual reconstruction loss of: {:06}/{:06} transactions'.format(now, i, reconstruction.size()[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiOs7Rx8Zvog",
        "outputId": "cd6bfba9-7161-4e75-a31e-9924b39ee116"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOG 20250508-01:21:47] collected individual reconstruction loss of: 000000/533009 transactions\n",
            "[LOG 20250508-01:21:52] collected individual reconstruction loss of: 100000/533009 transactions\n",
            "[LOG 20250508-01:21:58] collected individual reconstruction loss of: 200000/533009 transactions\n",
            "[LOG 20250508-01:22:03] collected individual reconstruction loss of: 300000/533009 transactions\n",
            "[LOG 20250508-01:22:09] collected individual reconstruction loss of: 400000/533009 transactions\n",
            "[LOG 20250508-01:22:14] collected individual reconstruction loss of: 500000/533009 transactions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init binary cross entropy errors\n",
        "reconstruction_loss_transaction = np.zeros(reconstruction.size()[0])\n",
        "\n",
        "# iterate over all detailed reconstructions\n",
        "for i in range(0, reconstruction.size()[0]):\n",
        "\n",
        "    # determine reconstruction loss - individual transactions\n",
        "    reconstruction_loss_transaction[i] = loss_function(reconstruction[i], data[i]).item()\n",
        "\n",
        "    if(i % 100000 == 0):\n",
        "\n",
        "        ### print conversion summary\n",
        "        now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
        "        print('[LOG {}] collected individual reconstruction loss of: {:06}/{:06} transactions'.format(now, i, reconstruction.size()[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h7SHdreZxZ0",
        "outputId": "bcabb8ea-26a4-45d5-b739-91a1d16e9a3b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOG 20250508-01:22:16] collected individual reconstruction loss of: 000000/533009 transactions\n",
            "[LOG 20250508-01:22:21] collected individual reconstruction loss of: 100000/533009 transactions\n",
            "[LOG 20250508-01:22:27] collected individual reconstruction loss of: 200000/533009 transactions\n",
            "[LOG 20250508-01:22:32] collected individual reconstruction loss of: 300000/533009 transactions\n",
            "[LOG 20250508-01:22:38] collected individual reconstruction loss of: 400000/533009 transactions\n",
            "[LOG 20250508-01:22:43] collected individual reconstruction loss of: 500000/533009 transactions\n"
          ]
        }
      ]
    }
  ]
}
